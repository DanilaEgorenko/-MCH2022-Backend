# :heavy_check_mark: [MoscowCityHack2022](https://moscityhack2022.innoagency.ru/#tacks) | Команда LowcatOrNot | Backend:ML :warning:

- **[Назад](..)**

## Работа с ML-моделью :factory:
  *Требования*
- *Windows* 10 / Ubuntu 20.04
- *python* 3.9+
- *python*-модули из [requirements.txt](./requirements.txt)


1. ```sudo apt-get install python3.9```
2. _Необязательно_ В Linux интерпретатор питона именуется __python3__, а в Windows - *__py__*.

    ```sudo alias py=python3```
3. _Необязательно_ Этот шаг можно пропустить, если не хотите использовать virtual environment. Переходите на пункт 5.

    ```sudo apt-get install python3.9-venv```
4. ```sudo py -m venv venv```
5. Windows: ```./venv/Scripts/activate```
    
    Linux: ```source venv/bin/activate```
6. ```sudo py -m pip install -r requirements```
    *Обращение к ML-модели*
7. Пример использования с комментариями см в demo.py(./demo.py).


В [lib/consts.py](./lib/consts.py) описаны базовые настройки парсера:
- *cache_dir* - директория, куда будет сохранена обученная модель, список игнорируемых хостов и кэш с разульатами обхода каждого отдельного url'а. 
- *dataset_dir* - директория с датасетами для обучения модели.
- *depricated_path* - путь к файлу .dat со структурой данных python list() со списком игнорируемых хостов.
- *train_dataset_path* - путь к датасету для обучения модели.
- *max_pages_per_site* - стандартное ограничение числа переходов по ссылками для _корневого_ url, чтобы не обходить сайты с кучей ссылок полностью.
- *sources* - список url сайтов, на которых указаны хосты, либо списки хостов, по которым будет создан итоговый список хостов, которые будут игнорироваться.
- *docs_extensions* - список расширений файлов (без точки), ссылки на которые будут сохранены парсером.


[create_text_dataset.py][./create_text_dataset.py] - скрипт сбора тренировочного датасета. Предварительно требуется таблица с url и отмеченной отраслью, в которой работает компания-владелец сайта. 


В [research.ipynb](./research.ipynb) описан процесс обучения модели предсказания отрасли производства, в которой работает компания-владелец сайта. Используется метод анализа текста TF-IDF, стеммер для русского языка, наивный баесовский алгоритм. Оптимальные параметры ищутся с помощью grid-search. Результаты mean для разных параметров и датасетов приведены.
